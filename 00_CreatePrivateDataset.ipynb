{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":36363,"databundleVersionId":4050810,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":4264054,"sourceType":"datasetVersion","datasetId":2406209},{"sourceId":13738049,"sourceType":"datasetVersion","datasetId":8741187}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Notebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# This will add the pydicom decompressors\n!pip install pydicom pylibjpeg pylibjpeg-libjpeg\n# Re-install a compatible version of numpy\n!pip install numpy==1.26.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## !!STOP AND RESTART SESSION!!\n## YOU ONLY NEED TO DO THIS ONCE\n## This makes sure the above dependencies are installed, and we're able to decompress images.\n## You can skip the first two cells after restarting.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport pydicom\nimport h5py\nimport cv2\nimport os\nfrom tqdm.auto import tqdm\nimport warnings\nimport json\nfrom sklearn.model_selection import train_test_split\n\nrandom_state = 24\n\n# Suppress the DeprecationWarning\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:26.702810Z","iopub.execute_input":"2025-11-17T05:10:26.703294Z","iopub.status.idle":"2025-11-17T05:10:29.476032Z","shell.execute_reply.started":"2025-11-17T05:10:26.703210Z","shell.execute_reply":"2025-11-17T05:10:29.475276Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set paths to the Kaggle data\nbbox_csv_path = '/kaggle/input/rsna-2022-cervical-spine-fracture-detection/train_bounding_boxes.csv'\n\n# We're using the clean version from the metadata dataset\nmeta_csv_path = '/kaggle/input/rsna-2022-spine-fracture-detection-metadata/meta_train_clean.csv'\n\n# This is the path to all 136GB of images\nimage_data_path = '/kaggle/input/rsna-2022-cervical-spine-fracture-detection/train_images/'\n\n# Set the output path\nhdf5_save_path = '/kaggle/working/fracture_dataset_subset.h5'\n\n# Settings\nimage_size = 256\nnegative_ratio = 3 # for each positive, we add 3 negative samples\nmax_boxes = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:44.854827Z","iopub.execute_input":"2025-11-17T05:10:44.855285Z","iopub.status.idle":"2025-11-17T05:10:44.860368Z","shell.execute_reply.started":"2025-11-17T05:10:44.855264Z","shell.execute_reply":"2025-11-17T05:10:44.859484Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load and view metadata\nmeta_df = pd.read_csv(meta_csv_path)\nmeta_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:47.545538Z","iopub.execute_input":"2025-11-17T05:10:47.545804Z","iopub.status.idle":"2025-11-17T05:10:48.514392Z","shell.execute_reply.started":"2025-11-17T05:10:47.545787Z","shell.execute_reply":"2025-11-17T05:10:48.513589Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID  Slice  ImageHeight  ImageWidth  SliceThickness  \\\n0  1.2.826.0.1.3680043.10001      1          512         512           0.625   \n1  1.2.826.0.1.3680043.10001      2          512         512           0.625   \n2  1.2.826.0.1.3680043.10001      3          512         512           0.625   \n\n   ImagePositionPatient_x  ImagePositionPatient_y  ImagePositionPatient_z  \n0                 -52.308                 -27.712                   7.282  \n1                 -52.308                 -27.712                   6.657  \n2                 -52.308                 -27.712                   6.032  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>Slice</th>\n      <th>ImageHeight</th>\n      <th>ImageWidth</th>\n      <th>SliceThickness</th>\n      <th>ImagePositionPatient_x</th>\n      <th>ImagePositionPatient_y</th>\n      <th>ImagePositionPatient_z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>1</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>7.282</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>2</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>6.657</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.10001</td>\n      <td>3</td>\n      <td>512</td>\n      <td>512</td>\n      <td>0.625</td>\n      <td>-52.308</td>\n      <td>-27.712</td>\n      <td>6.032</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Load and view bounding box data\nbbox_df = pd.read_csv(bbox_csv_path)\nbbox_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:50.289478Z","iopub.execute_input":"2025-11-17T05:10:50.289754Z","iopub.status.idle":"2025-11-17T05:10:50.320814Z","shell.execute_reply.started":"2025-11-17T05:10:50.289733Z","shell.execute_reply":"2025-11-17T05:10:50.319911Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            StudyInstanceUID          x          y     width    height  \\\n0  1.2.826.0.1.3680043.10051  219.27715  216.71419  17.30440  20.38517   \n1  1.2.826.0.1.3680043.10051  221.56460  216.71419  17.87844  25.24362   \n2  1.2.826.0.1.3680043.10051  216.82151  221.62546  27.00959  26.37454   \n\n   slice_number  \n0           133  \n1           134  \n2           135  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>x</th>\n      <th>y</th>\n      <th>width</th>\n      <th>height</th>\n      <th>slice_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.10051</td>\n      <td>219.27715</td>\n      <td>216.71419</td>\n      <td>17.30440</td>\n      <td>20.38517</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.10051</td>\n      <td>221.56460</td>\n      <td>216.71419</td>\n      <td>17.87844</td>\n      <td>25.24362</td>\n      <td>134</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.10051</td>\n      <td>216.82151</td>\n      <td>221.62546</td>\n      <td>27.00959</td>\n      <td>26.37454</td>\n      <td>135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print('Loading and merging metadata...')\n\n# Rename slice columns to match\nmeta_df = meta_df.rename(columns={'Slice': 'SliceNumber'})\nbbox_df = bbox_df.rename(columns={'slice_number': 'SliceNumber'})\n\n# Group by slice and aggregate bboxes into a list [x, y, width, height]\nprint('Aggregating bounding boxes per slice...')\nbbox_grouped = bbox_df.groupby(['StudyInstanceUID', 'SliceNumber'])[['x', 'y', 'width', 'height']].apply(\n    lambda x: x.values.tolist()\n).reset_index(name='bboxes')\nprint(f\"Found {len(bbox_grouped)} slices with bounding boxes.\")\n\n# Merge all slices with the bounding box data\nall_slices_df = pd.merge(\n    meta_df,\n    bbox_grouped,\n    on=['StudyInstanceUID', 'SliceNumber'],\n    how='left'\n)\n\n# Create final labels\n# 'is_positive' is 1 if 'bboxes' is not NaN, 0 otherwise\nall_slices_df['is_positive'] = all_slices_df['bboxes'].notna().astype(int)\n\n# Fill NaN in 'bboxes' with an empty list for consistency\nall_slices_df['bboxes'] = all_slices_df['bboxes'].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n\nprint('Metadata merge complete.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:52.297753Z","iopub.execute_input":"2025-11-17T05:10:52.298137Z","iopub.status.idle":"2025-11-17T05:10:53.244424Z","shell.execute_reply.started":"2025-11-17T05:10:52.298094Z","shell.execute_reply":"2025-11-17T05:10:53.243586Z"}},"outputs":[{"name":"stdout","text":"Loading and merging metadata...\nAggregating bounding boxes per slice...\nFound 7217 slices with bounding boxes.\nMetadata merge complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print('Creating positive/negative subset...')\n\npositive_samples = all_slices_df[all_slices_df['is_positive'] == 1]\nnegative_samples = all_slices_df[all_slices_df['is_positive'] == 0]\n\nnum_positives = len(positive_samples)\nnum_negatives_to_sample = min(\n    int(num_positives * negative_ratio),\n    len(negative_samples)\n)\n\nprint(f'Total positive slices: {num_positives}')\nprint(f'Sampling {num_negatives_to_sample} negative slices (Ratio: {negative_ratio})')\n\nnegative_samples_subset = negative_samples.sample(\n    n=num_negatives_to_sample,\n    random_state=random_state\n)\n\nfinal_subset_df = pd.concat(\n    [positive_samples, negative_samples_subset]\n).sample(frac=1, random_state=random_state).reset_index(drop=True)\n\nprint(f'\\n--- Total samples to process: {len(final_subset_df)} ---')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:55.162674Z","iopub.execute_input":"2025-11-17T05:10:55.163008Z","iopub.status.idle":"2025-11-17T05:10:55.252311Z","shell.execute_reply.started":"2025-11-17T05:10:55.162990Z","shell.execute_reply":"2025-11-17T05:10:55.251376Z"}},"outputs":[{"name":"stdout","text":"Creating positive/negative subset...\nTotal positive slices: 7217\nSampling 21651 negative slices (Ratio: 3)\n\n--- Total samples to process: 28868 ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create a train/validation/test split\n\n# Create a 70% train, 15% validation, 15% test split.\n# We stratify on 'is_positive' to ensure all sets have a similar fracture ratio.\n\n# First, split into 70% train and 30% (val + test)\ntrain_df, val_test_df = train_test_split(\n    final_subset_df,\n    test_size=0.30, # 30% will be for val+test\n    random_state=42,\n    stratify=final_subset_df['is_positive']\n)\n\n# Next, split the 30% (val + test) in half (15% and 15%)\nval_df, test_df = train_test_split(\n    val_test_df,\n    test_size=0.50, # 50% of the 30% -> 15% of the total\n    random_state=42,\n    stratify=val_test_df['is_positive']\n)\n\n# Add a 'split' column to each DataFrame\n# 0 = train\n# 1 = validation\n# 2 = test\ntrain_df['split'] = 0\nval_df['split'] = 1\ntest_df['split'] = 2\n\n# Recombine them into one final DataFrame ---\nfinal_subset_df = pd.concat([train_df, val_df, test_df]).sort_index()\n\nprint('\\n--- Final Dataset with Splits ---')\nprint(f'Total samples: {len(final_subset_df)}')\nprint(f'  Training samples:   {len(train_df)}')\nprint(f'  Validation samples: {len(val_df)}')\nprint(f'  Test samples:       {len(test_df)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:10:57.976778Z","iopub.execute_input":"2025-11-17T05:10:57.977375Z","iopub.status.idle":"2025-11-17T05:10:58.010608Z","shell.execute_reply.started":"2025-11-17T05:10:57.977341Z","shell.execute_reply":"2025-11-17T05:10:58.009787Z"}},"outputs":[{"name":"stdout","text":"\n--- Final Dataset with Splits ---\nTotal samples: 28868\n  Training samples:   20207\n  Validation samples: 4330\n  Test samples:       4331\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# final_subset_df.head()\nfinal_subset_df['bboxes'] # before box scaling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:11:00.720880Z","iopub.execute_input":"2025-11-17T05:11:00.721196Z","iopub.status.idle":"2025-11-17T05:11:00.729437Z","shell.execute_reply.started":"2025-11-17T05:11:00.721175Z","shell.execute_reply":"2025-11-17T05:11:00.728533Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0                                    []\n1                                    []\n2                                    []\n3        [[141.0, 152.0, 235.0, 106.0]]\n4                                    []\n                      ...              \n28863                                []\n28864                                []\n28865                                []\n28866     [[195.0, 171.0, 115.0, 67.0]]\n28867                                []\nName: bboxes, Length: 28868, dtype: object"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Scale bounding boxes from original 512x512 to 256x256\n# We can divide each box coordinate by 2, or multiply by half\nscale_factor = 0.5\n\ndef scale_bboxes(bbox_list, scale_factor):\n    if not bbox_list:  # for negative sample\n        return []\n        \n    scaled_boxes = []\n    for box in bbox_list:\n        # Scale each coordinate [x, y, w, h]\n        scaled_box = [coord * scale_factor for coord in box]\n        scaled_boxes.append(scaled_box)\n        \n    return scaled_boxes\n\n# Apply this function to the 'bboxes' column\nfinal_subset_df['bboxes'] = final_subset_df['bboxes'].apply(\n    lambda x: scale_bboxes(x, scale_factor)\n)\n\nprint('Bounding boxes successfully scaled down for 256x256 size.')\nfinal_subset_df['bboxes'] # after box scaling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:11:02.766708Z","iopub.execute_input":"2025-11-17T05:11:02.767435Z","iopub.status.idle":"2025-11-17T05:11:02.799940Z","shell.execute_reply.started":"2025-11-17T05:11:02.767412Z","shell.execute_reply":"2025-11-17T05:11:02.799099Z"}},"outputs":[{"name":"stdout","text":"Bounding boxes successfully scaled down for 256x256 size.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0                                 []\n1                                 []\n2                                 []\n3        [[70.5, 76.0, 117.5, 53.0]]\n4                                 []\n                    ...             \n28863                             []\n28864                             []\n28865                             []\n28866     [[97.5, 85.5, 57.5, 33.5]]\n28867                             []\nName: bboxes, Length: 28868, dtype: object"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Image preprocessing function\ndef load_and_process_dicom(uid, slice_num, target_size):\n    # Build the local file path\n    file_path = f'{image_data_path}/{uid}/{slice_num}.dcm'\n    \n    ds = pydicom.dcmread(file_path)\n    \n    # Get pixel array\n    img = ds.pixel_array.astype(np.float32)\n    \n    # Perform min-max normalization (scales pixel intensity to range [0,1])\n    img_min = np.min(img)\n    img_max = np.max(img)\n    if img_max > img_min:\n        img = (img - img_min) / (img_max - img_min)\n    else:\n        img = np.zeros(img.shape) # Handle black images\n    \n    # Resize\n    img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n    \n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:11:10.361020Z","iopub.execute_input":"2025-11-17T05:11:10.361707Z","iopub.status.idle":"2025-11-17T05:11:10.367001Z","shell.execute_reply.started":"2025-11-17T05:11:10.361646Z","shell.execute_reply":"2025-11-17T05:11:10.366208Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"## This cell was made using Google Gemini AI ##\n\nnum_samples = len(final_subset_df)\nprint(f\"Creating HDF5 file at {hdf5_save_path} with {num_samples} total samples...\")\n\n# We'll write in chunks of 32 images at a time\nimage_chunk = (32, image_size, image_size) \nbbox_chunk = (32, max_boxes, 4)\ntext_chunk = (512,) # A good chunk size for 1D arrays\n\nwith h5py.File(hdf5_save_path, 'w') as hf:\n    \n    # --- Create datasets ---\n    dset_images = hf.create_dataset('images', shape=(num_samples, image_size, image_size), dtype='f4', chunks=image_chunk)\n    dset_labels = hf.create_dataset('labels', shape=(num_samples,), dtype='i1', chunks=text_chunk)\n    dset_bboxes = hf.create_dataset('bboxes', shape=(num_samples, max_boxes, 4), dtype='f4', fillvalue=-1.0, chunks=bbox_chunk)\n    dt_str = h5py.special_dtype(vlen=str)\n    dset_uid = hf.create_dataset('StudyInstanceUID', (num_samples,), dtype=dt_str, chunks=text_chunk)\n    dset_slice = hf.create_dataset('SliceNumber', (num_samples,), dtype=dt_str, chunks=text_chunk)\n    dset_split = hf.create_dataset('split', shape=(num_samples,), dtype='i1', chunks=text_chunk)\n\n    # --- Start the processing loop ---\n    print(\"Starting processing loop... This will be fast!\")\n    \n    for idx, row in tqdm(final_subset_df.iterrows(), total=num_samples, desc=\"Processing slices\"):\n        try:\n            # 1. Load and process the image from the local disk\n            img = load_and_process_dicom(\n                row['StudyInstanceUID'], \n                row['SliceNumber'], \n                image_size\n            )\n            \n            # 2. Save data to HDF5 file\n            dset_images[idx] = img\n            dset_labels[idx] = row['is_positive']\n            dset_uid[idx] = row['StudyInstanceUID']\n            dset_slice[idx] = str(row['SliceNumber'])\n            dset_split[idx] = row['split']\n            \n            # 3. Process and save bounding boxes\n            bboxes = row['bboxes']\n            num_boxes = min(len(bboxes), max_boxes)\n            \n            if num_boxes > 0:\n                dset_bboxes[idx, :num_boxes, :] = np.array(bboxes[:num_boxes])\n                \n        except Exception as e:\n            # This will catch any corrupted files\n            print(f\"\\n[Warning] Failed to process slice {row['StudyInstanceUID']}/{row['SliceNumber']}: {e}\")\n\nprint(\"\\n--- HDF5 file creation complete! ---\")\nprint(f\"File saved to: {hdf5_save_path}\")\n!ls -lh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:11:15.953684Z","iopub.execute_input":"2025-11-17T05:11:15.953971Z","iopub.status.idle":"2025-11-17T05:22:10.210865Z","shell.execute_reply.started":"2025-11-17T05:11:15.953952Z","shell.execute_reply":"2025-11-17T05:22:10.209435Z"}},"outputs":[{"name":"stdout","text":"Creating HDF5 file at /kaggle/working/fracture_dataset_subset.h5 with 28868 total samples...\nStarting processing loop... This will be fast!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing slices:   0%|          | 0/28868 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06327f77989d4e1886c9254f64676e80"}},"metadata":{}},{"name":"stdout","text":"\n--- HDF5 file creation complete! ---\nFile saved to: /kaggle/working/fracture_dataset_subset.h5\ntotal 7.1G\n-rw-r--r-- 1 root root 7.1G Nov 17 05:22 fracture_dataset_subset.h5\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Load API credentials from your private api key dataset\n# If you do not have this private dataset setup, see project README for instructions\n# Doing this to work around Secret add-ons not working\n\nprivate_secret_dataset_name = 'DATASET_NAME' ### REPLACE WITH YOUR PRIVATE DATASET NAME ###\n\n\nCREDENTIALS_PATH = f'/kaggle/input/{private_secret_dataset_name}/kaggle.json'\n\n# Create the hidden .kaggle directory\n!mkdir -p ~/.kaggle\n\n# Copy your key from the private dataset to the correct location\n!cp '{CREDENTIALS_PATH}' ~/.kaggle/kaggle.json\n\n# Set the correct permissions for the file\n!chmod 600 ~/.kaggle/kaggle.json\n\nprint('Kaggle API credentials are now in place.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:22:21.472439Z","iopub.execute_input":"2025-11-17T05:22:21.472855Z","iopub.status.idle":"2025-11-17T05:22:21.890214Z","shell.execute_reply.started":"2025-11-17T05:22:21.472824Z","shell.execute_reply":"2025-11-17T05:22:21.889179Z"}},"outputs":[{"name":"stdout","text":"Kaggle API credentials are now in place.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Create the private Kaggle dataset\n\nkaggle_username = 'KAGGLE_USERNAME' ### Set KAGGLE_USERNAME ###\n\n\n# Define your dataset metadata\ndataset_metadata = {\n  \"title\": \"RSNA 2022 HDF5 Subset\",\n  \"id\": f\"{kaggle_username}/rsna-2022-hdf5-subset\", \n  \"licenses\": [\n    {\n      \"name\": \"CC0-1.0\"\n    }\n  ]\n}\n\n# Write the metadata file\nwith open('/kaggle/working/dataset-metadata.json', 'w') as f:\n    json.dump(dataset_metadata, f)\n\n# Run the create command\n# You may have to wait a few minutes for the dataset to fully load to the Kaggle API system\nprint('Starting dataset creation... This will upload 7.05 GB.')\n!kaggle datasets create -p /kaggle/working/ -r zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T05:22:27.162250Z","iopub.execute_input":"2025-11-17T05:22:27.162592Z","iopub.status.idle":"2025-11-17T05:25:23.950149Z","shell.execute_reply.started":"2025-11-17T05:22:27.162564Z","shell.execute_reply":"2025-11-17T05:25:23.949013Z"}},"outputs":[{"name":"stdout","text":"Starting dataset creation... This will upload 7.05 GB.\nStarting upload for file fracture_dataset_subset.h5\n100%|██████████████████████████████████████| 7.06G/7.06G [02:52<00:00, 43.9MB/s]\nUpload successful: fracture_dataset_subset.h5 (7GB)\nStarting upload for file .virtual_documents.zip\n100%|█████████████████████████████████████████| 22.0/22.0 [00:00<00:00, 49.4B/s]\nUpload successful: .virtual_documents.zip (22B)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/andymalinsky/rsna-2022-hdf5-subset\n","output_type":"stream"}],"execution_count":14}]}